import multiprocessing
import time
from crawler_wiki import crawl_wikipedia
from crawler_web_thematic import crawl_web_topic

# --- CONFIGURATION DES 5 CRAWLERS ---
CRAWLER_TASKS = [
    # 1. CRAWLER WIKIPEDIA (Autorit√© SRAS)
    {
        "type": "wiki",
        "name": "WIKI-1",
        # Pages de d√©part pour une exploration cibl√©e
        "args": (["Intelligence artificielle", "Crise climatique", "Technologies de l'information"], "WIKI-1")
    },
    
    # 2. CRAWLERS ACTUALIT√âS (2 Processus)
    {
        "type": "web",
        "name": "ACTU-1",
        # Th√®me sp√©cifique pour ce processus
        "args": ("Actualit√©s politiques et √©conomiques r√©centes France", "ACTU-1", 100) 
    },
    {
        "type": "web",
        "name": "ACTU-2",
        "args": ("Derni√®res nouvelles scientifiques et m√©dicales", "ACTU-2", 100)
    },

    # 3. CRAWLERS DIVERTISSEMENT (2 Processus)
    {
        "type": "web",
        "name": "DIVERT-1",
        "args": ("Nouveaut√©s films et s√©ries t√©l√©vis√©es 2024", "DIVERT-1", 75)
    },
    {
        "type": "web",
        "name": "DIVERT-2",
        "args": ("Actualit√©s des jeux vid√©o et e-sport", "DIVERT-2", 75)
    }
]

def main():
    print("üé¨ D√âMARRAGE DE NEOSEEK - 5 CRAWLERS EN PARALL√àLE")
    processes = []
    
    # Cr√©ation des processus
    for task in CRAWLER_TASKS:
        if task["type"] == "wiki":
            p = multiprocessing.Process(target=crawl_wikipedia, args=task["args"])
        elif task["type"] == "web":
            p = multiprocessing.Process(target=crawl_web_topic, args=task["args"])
        
        processes.append(p)
        p.start()
        # Petite pause pour √©viter une saturation imm√©diate au d√©marrage
        time.sleep(0.5) 

    # Attente de la fin de tous les processus
    for p in processes:
        p.join()

    print("\n TOUS LES CRAWLERS ONT TERMIN√â LEUR T√ÇCHE.")
    print("Projet NeoSeek mis √† jour avec la nouvelle strat√©gie d'exploration.")

if __name__ == '__main__':
    main()